{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.ndimage import gaussian_filter, zoom, sobel, maximum_filter, laplace, gaussian_laplace\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from scipy.optimize import least_squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f81d49",
   "metadata": {},
   "source": [
    "Just For Fun i tried to implement the feature detection method as same as used in the Pizzaro paper i.e. harris interest points detector at multiscale this multiscale was created by decomposing each image into 5 levels with sigma value 1.6 and any level further then 5 was too blur, first the initial image from the pyramid in the previous scale is blurred then zoomed in with the factors of 2 and the gaussian blurr was used the same across all the levels as the image was already downsampled at each level, i have attached the outcomes of the pyramids in the Pyramids Folder as this algorithm was working but the computational time was too high it took me more then 2 hours to finish running this algorithm since there are a lots of loops in the code i didnt had  much time to optimize it for computational efficiency so i shifted from this feature finding method to SIFT algorithm but i would still like to know how can i improve this code below for it to work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iam Following the Paper's method so here iam decomposing each image into 5 levels with sigma value 1.6 and any level further then 5 was too blur \n",
    "# #first the initial image from the pyramid in the previous scale is blurred then zoomed in with the factors of 2,\n",
    "# img_1 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/1.tif\")\n",
    "# img_2 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/2.tif\")\n",
    "# img_3 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/3.tif\")\n",
    "# img_4 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/4.tif\")\n",
    "# img_5 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/5.tif\")\n",
    "# img_6 = mpimg.imread(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/6.tif\")\n",
    "\n",
    "# levels = 5\n",
    "\n",
    "# def Build_pyramid(image, sigma = 1.6):\n",
    "#     pyramid = [image.copy()]\n",
    "#     for level in range(1,levels):\n",
    "#         blurred = gaussian_filter(pyramid[-1], sigma = sigma)\n",
    "#         downsampled = zoom(blurred, 0.5, order=1)\n",
    "#         pyramid.append(downsampled)\n",
    "#     return pyramid \n",
    "\n",
    "\n",
    "# Pyramid_im1 = Build_pyramid(img_1)\n",
    "# Pyramid_im2 = Build_pyramid(img_2)\n",
    "# Pyramid_im3 = Build_pyramid(img_3)\n",
    "# Pyramid_im4 = Build_pyramid(img_4)\n",
    "# Pyramid_im5 = Build_pyramid(img_5)\n",
    "# Pyramid_im6 = Build_pyramid(img_6)\n",
    "\n",
    "# Pyramids = [Pyramid_im1,Pyramid_im2,Pyramid_im3,Pyramid_im4,Pyramid_im5,Pyramid_im6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0771ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for plotting and checking how the levels look, all the level plots for all the 6 images is in the folder Pyramids Please take a look at it \n",
    "\n",
    "# plt.figure(figsize=(25, 10))\n",
    "# for i in range(5):\n",
    "#     plt.subplot(2, 3, i+1) \n",
    "#     plt.imshow(Pyramid_im6[i], cmap='gray')  \n",
    "#     plt.title(f\"Level {i}\")\n",
    "# plt.tight_layout() \n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Developing the Harris interest point algo and deploying it on all the scales in this pyramid \n",
    "# def Harris_interest_points(image, sigma_d=1.1, sigma_I=1.3):\n",
    "#     smoothed = gaussian_filter(image, sigma=sigma_d, mode='constant')\n",
    "#     lx = sobel(smoothed, axis=1, mode='constant')\n",
    "#     ly = sobel(smoothed, axis=0, mode='constant')\n",
    "#     lxy = lx*ly\n",
    "#     Ix2_smoothed = gaussian_filter(lx, sigma=sigma_I, mode='constant')\n",
    "#     Iy2_smoothed = gaussian_filter(ly, sigma=sigma_I, mode='constant')\n",
    "#     Ixy_smoothed = gaussian_filter(lxy, sigma=sigma_I, mode='constant')\n",
    "#     meu_11 = sigma_d * Ix2_smoothed\n",
    "#     meu_12 = sigma_d * Ixy_smoothed\n",
    "#     meu_21 = sigma_d * Ixy_smoothed\n",
    "#     meu_22 = sigma_d * Iy2_smoothed\n",
    "#     height,width = image.shape\n",
    "#     eigenvalues = np.zeros((height,width,2))\n",
    "#     for i in range(height):\n",
    "#         for j in range(width):\n",
    "#             M = np.array([[meu_11[i,j], meu_12[i,j]],\n",
    "#                           [meu_21[i,j], meu_22[i,j]]])\n",
    "#             eig_val = np.linalg.eigvals(M)\n",
    "#             eigenvalues[i,j] = np.sort(eig_val)\n",
    "#     return eigenvalues\n",
    "\n",
    "# tentative_features = []\n",
    "\n",
    "# for PY_index ,pyramid in enumerate(Pyramids):\n",
    "#   for level,image in enumerate(pyramid):\n",
    "#      height,width = image.shape\n",
    "#      eigenvalues = Harris_interest_points(image)\n",
    "#      for i in range(height):\n",
    "#         for j in range(width):\n",
    "#             if eigenvalues[i,j,0] > 0 and eigenvalues[i,j,1] > 0:\n",
    "#                 tentative_features.append((i,j,level,PY_index))\n",
    "\n",
    "# print(tentative_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def scale_normalized_Laplacian(image, thresh, image_num, sigma=1.4,):\n",
    "#     interest_points = []\n",
    "#     for index,value in enumerate(tentative_features):\n",
    "#         i,j,l,py = value\n",
    "        \n",
    "#         if py == image_num:\n",
    "#             response = []\n",
    "#             for level,im in enumerate(image):\n",
    "#                 print(\"here in loop2\")\n",
    "#                 if l == level:\n",
    "#                   i_n,j_n = i,j\n",
    "#                 elif l>level:\n",
    "#                     factor = 2**(l-level)\n",
    "#                     i_n,j_n = i*factor,j*factor\n",
    "#                 else:\n",
    "#                     factor = 2**(level-l)\n",
    "#                     i_n,j_n = i//factor,j//factor\n",
    "#                     print(\"its3\", i_n,j_n)\n",
    "                \n",
    "#                 if (0 <= i_n < im.shape[0] and 0 <= j_n < im.shape[1]):\n",
    "#                     print(\"yup iam exicuted\")\n",
    "#                     curr_sigma = sigma*(2**level)\n",
    "#                     Laplacian = gaussian_laplace(im, sigma=curr_sigma, mode='constant')\n",
    "#                     val_L = abs(((curr_sigma**2)*Laplacian[i_n,j_n]))\n",
    "#                     response.append((val_L,level))\n",
    "#             print(\"itsherenow\")\n",
    "#             max_response = max(response, key=lambda item: response[0])\n",
    "#             __,charac_scale = max_response\n",
    "#             eigenvalues_charac_scale = Harris_interest_points(image[charac_scale])\n",
    "#             if l == 0:\n",
    "#                 i_f,j_f = i,j\n",
    "#             elif l>0:\n",
    "#                 factor = 2**(l)\n",
    "#                 i_f,j_f = i*factor,j*factor\n",
    "\n",
    "#             min_eigval = np.min(eigenvalues_charac_scale[i_f,j_f])\n",
    "\n",
    "#             if min_eigval < thresh: \n",
    "#                     print(\"here in loop1\")\n",
    "#                     interest_points.append((i_f,j_f,charac_scale))\n",
    "\n",
    "\n",
    "#     return interest_points\n",
    "\n",
    "\n",
    "# interest_points = scale_normalized_Laplacian(Pyramid_im1,20,0)\n",
    "\n",
    "# print(interest_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d45e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        img = plt.imread(path)\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def normalize_image(image):\n",
    "    img_float = image.astype(np.float64)\n",
    "    \n",
    "    mean_val = np.mean(img_float)\n",
    "    std_val = np.std(img_float)\n",
    "    \n",
    "    if std_val > 0:\n",
    "        normalized = (img_float - mean_val) / std_val\n",
    "    else:\n",
    "        normalized = img_float - mean_val\n",
    "    \n",
    "    normalized = ((normalized - normalized.min()) / \n",
    "                  (normalized.max() - normalized.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "img_1 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/1.tif\"))\n",
    "img_2 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/2.tif\"))\n",
    "img_3 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/3.tif\"))\n",
    "img_4 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/4.tif\"))\n",
    "img_5 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/5.tif\"))\n",
    "img_6 = normalize_image(load_image(\"C:/Users/shalh/OneDrive/Desktop/Underwater Image/6.tif\"))\n",
    "\n",
    "sift_det =  cv2.SIFT.create()\n",
    "\n",
    "fp1, desp1 = sift_det.detectAndCompute(img_1,None)\n",
    "fp2, desp2 = sift_det.detectAndCompute(img_2, None)\n",
    "fp3, desp3 = sift_det.detectAndCompute(img_3,None)\n",
    "fp4, desp4 = sift_det.detectAndCompute(img_4, None)\n",
    "fp5, desp5 = sift_det.detectAndCompute(img_5,None)\n",
    "fp6, desp6 = sift_det.detectAndCompute(img_6, None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "trees = dict(algorithm=FLANN_INDEX_KDTREE)\n",
    "search = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(trees,search)\n",
    "\n",
    "matches12 = flann.knnMatch(desp1,desp2,k=2)\n",
    "matches23 = flann.knnMatch(desp2,desp3,k=2)\n",
    "matches34 = flann.knnMatch(desp3,desp4,k=2)\n",
    "matches45 = flann.knnMatch(desp4,desp5,k=2)\n",
    "matches56 = flann.knnMatch(desp5,desp6,k=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(im12_matches, cmap='gray')\n",
    "plt.title(\"matched points\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_matches12 = []\n",
    "sorted_matches23 = []\n",
    "sorted_matches34 = []\n",
    "sorted_matches45 = []\n",
    "sorted_matches56 = []\n",
    "\n",
    "\n",
    "matches = [matches12,matches23,matches34,matches45,matches56]\n",
    "\n",
    "for i,j in matches12:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches12.append(i)\n",
    "for i,j in matches23:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches23.append(i)\n",
    "for i,j in matches34:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches34.append(i)\n",
    "for i,j in matches45:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches45.append(i)\n",
    "for i,j in matches56:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches56.append(i)\n",
    "\n",
    "im12_matches = cv2.drawMatches(img_1, fp1, img_2, fp2, sorted_matches12, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im23_matches = cv2.drawMatches(img_2, fp2, img_3, fp3, sorted_matches23, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im34_matches = cv2.drawMatches(img_3, fp3, img_4, fp4, sorted_matches34, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im45_matches = cv2.drawMatches(img_4, fp4, img_5, fp5, sorted_matches45, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im56_matches = cv2.drawMatches(img_5, fp5, img_6, fp6, sorted_matches56, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "matched12_xy = []\n",
    "matched23_xy = []\n",
    "matched34_xy = []\n",
    "matched45_xy = []\n",
    "matched56_xy = []\n",
    "for match in sorted_matches12:\n",
    "    pt1 = fp1[match.queryIdx].pt\n",
    "    pt2 = fp2[match.trainIdx].pt\n",
    "    matched12_xy.append((pt1, pt2))\n",
    "for match in sorted_matches23:\n",
    "    pt2 = fp2[match.queryIdx].pt\n",
    "    pt3 = fp3[match.trainIdx].pt\n",
    "    matched23_xy.append((pt2, pt3))\n",
    "for match in sorted_matches34:\n",
    "    pt3 = fp3[match.queryIdx].pt\n",
    "    pt4 = fp4[match.trainIdx].pt\n",
    "    matched34_xy.append((pt3, pt4))\n",
    "for match in sorted_matches45:\n",
    "    pt4 = fp4[match.queryIdx].pt\n",
    "    pt5 = fp5[match.trainIdx].pt\n",
    "    matched45_xy.append((pt4, pt5))\n",
    "for match in sorted_matches56:\n",
    "    pt5 = fp5[match.queryIdx].pt\n",
    "    pt6 = fp6[match.trainIdx].pt\n",
    "    matched56_xy.append((pt5, pt6))\n",
    "\n",
    "im1_pt,im2_pt = matched12_xy[0]\n",
    "\n",
    "print(np.float32([matched12_xy[i][1] for i,j in enumerate(matched12_xy)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # LMS algo for Homographies calculation \n",
    "\n",
    "# def LMS_homography_calc(pairs, trials=1000):\n",
    "#     best_H = None\n",
    "#     best_median_residual = np.inf\n",
    "#     im1_pt_all = np.float32([pairs[i][0] for i,j in enumerate(pairs)]).reshape(-1, 1, 2)\n",
    "#     im2_pt_all = np.float32([pairs[i][1] for i,j in enumerate(pairs)]).reshape(-1, 1, 2)\n",
    "\n",
    "#     for trial in range(trials):\n",
    "#         index = np.random.choice(len(pairs), 4, replace=False)\n",
    "#         im1_pt = np.float32([pairs[i][0] for i in index])\n",
    "#         im2_pt = np.float32([pairs[i][1] for i in index])\n",
    "\n",
    "#         H = cv2.getPerspectiveTransform(im1_pt,im2_pt)\n",
    "#         transformed_im1pt = cv2.perspectiveTransform(im1_pt_all, H)\n",
    "#         residual_err = np.sqrt(np.sum((transformed_im1pt - im2_pt_all)**2, axis=2)).flatten()\n",
    "#         median_residual = np.median(residual_err)\n",
    "\n",
    "#         if median_residual < best_median_residual:\n",
    "#             best_H = H\n",
    "#             best_median_residual = median_residual\n",
    "\n",
    "#     return best_H, best_median_residual\n",
    "\n",
    "# H12, best_median_res12 = LMS_homography_calc(matched12_xy, trials=1000)\n",
    "# H23, best_median_res23 = LMS_homography_calc(matched23_xy, trials=1000)\n",
    "# H34, best_median_res34 = LMS_homography_calc(matched34_xy, trials=1000)\n",
    "# H45, best_median_res45 = LMS_homography_calc(matched45_xy, trials=1000)\n",
    "# H56, best_median_res56 = LMS_homography_calc(matched56_xy, trials=1000)\n",
    "\n",
    "# print(H12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f719b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pts(pairs):\n",
    "    im1_pt_all = np.float32([pairs[i][0] for i,j in enumerate(pairs)]).reshape(-1, 1, 2)\n",
    "    im2_pt_all = np.float32([pairs[i][1] for i,j in enumerate(pairs)]).reshape(-1, 1, 2)\n",
    "    return im1_pt_all,im2_pt_all\n",
    "\n",
    "\n",
    "src_pts12,dst_pts12 = pts(matched12_xy)\n",
    "src_pts23,dst_pts23 = pts(matched23_xy)\n",
    "src_pts34,dst_pts34 = pts(matched34_xy)\n",
    "src_pts45,dst_pts45 = pts(matched45_xy)\n",
    "src_pts56,dst_pts56 = pts(matched56_xy)\n",
    "\n",
    "\n",
    "\n",
    "H12i, mask12 = cv2.findHomography(src_pts12, dst_pts12, cv2.RANSAC)\n",
    "H23i, mask23 = cv2.findHomography(src_pts23, dst_pts23, cv2.RANSAC)\n",
    "H34i, mask34 = cv2.findHomography(src_pts34, dst_pts34, cv2.RANSAC)\n",
    "H45i, mask45 = cv2.findHomography(src_pts45, dst_pts45, cv2.RANSAC)\n",
    "H56i, mask56 = cv2.findHomography(src_pts56, dst_pts56, cv2.RANSAC)\n",
    "print(\"h12i\", H12i)\n",
    "\n",
    "inlier_src12, inlier_dst12 = src_pts12[mask12.ravel() == 1], dst_pts12[mask12.ravel() == 1]\n",
    "inlier_src23, inlier_dst23 = src_pts23[mask23.ravel() == 1], dst_pts23[mask23.ravel() == 1]\n",
    "inlier_src34, inlier_dst34 = src_pts34[mask34.ravel() == 1], dst_pts34[mask34.ravel() == 1]\n",
    "inlier_src45, inlier_dst45 = src_pts45[mask45.ravel() == 1], dst_pts45[mask45.ravel() == 1]\n",
    "inlier_src56, inlier_dst56 = src_pts56[mask56.ravel() == 1], dst_pts56[mask56.ravel() == 1]\n",
    "\n",
    "\n",
    "def residual_function(h_params, src_pts, dst_pts):\n",
    "    H = np.append(h_params, 1).reshape(3, 3)\n",
    "    \n",
    "    src_homo = np.column_stack([src_pts.reshape(-1, 2), np.ones(len(src_pts.reshape(-1, 2)))])\n",
    "    projected_homo = (H @ src_homo.T).T\n",
    "    projected_2d = projected_homo[:, :2] / projected_homo[:, 2:3]\n",
    "    \n",
    "    return (projected_2d - dst_pts.reshape(-1, 2)).flatten()\n",
    "\n",
    "def LM(H_init, src_pts, dst_pts):\n",
    "    h_params = H_init.flatten()[:8]\n",
    "    \n",
    "    result = least_squares(residual_function, h_params, \n",
    "                          args=(src_pts, dst_pts), \n",
    "                          method='lm')\n",
    "    \n",
    "    H_refined = np.append(result.x, 1).reshape(3, 3)\n",
    "    return H_refined\n",
    "\n",
    "H12 = LM(H12i, inlier_src12, inlier_dst12)\n",
    "H23 = LM(H23i, inlier_src23, inlier_dst23)\n",
    "H34 = LM(H34i, inlier_src34, inlier_dst34)\n",
    "H45 = LM(H45i, inlier_src45, inlier_dst45)\n",
    "H56 = LM(H56i, inlier_src56, inlier_dst56)\n",
    "print(H12)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba458a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_image_stitching(images,homographies):\n",
    "    result = images[0].copy()\n",
    "    canvas_w = images[0].shape[1]*3\n",
    "    canvas_h = images[0].shape[0]*3\n",
    "\n",
    "    canvas = np.zeros((canvas_h, canvas_w), dtype=np.uint8)\n",
    "\n",
    "    center_transform = np.array([[1, 0, canvas_w//2 - images[0].shape[1]//2],\n",
    "                                 [0, 1, canvas_h//2 - images[0].shape[0]//2],\n",
    "                                 [0, 0, 1]], dtype=np.float32)\n",
    "    \n",
    "    # Place first image in center\n",
    "    first_H = center_transform\n",
    "    warped_first = cv2.warpPerspective(images[0],  first_H, (canvas_w, canvas_h))\n",
    "    canvas = warped_first.copy()\n",
    "\n",
    "    # x_offset = canvas_w//4\n",
    "    # y_offset = canvas_h//4\n",
    "    # canvas[y_offset:y_offset+images[0].shape[0], x_offset:x_offset+images[0].shape[1]] = images[0]\n",
    "\n",
    "    cum_H = first_H \n",
    "    for i in range(1,len(images)):\n",
    "        cum_H =  np.dot(cum_H, homographies[i-1])\n",
    "        warped = cv2.warpPerspective(images[i], cum_H, (canvas_w, canvas_h))\n",
    "        alpha = 0.5\n",
    "        beta = 1.0 - alpha\n",
    "        warped = cv2.addWeighted(warped_first, alpha, warped, beta, 0)\n",
    "      \n",
    "        mask = warped > 0\n",
    "        canvas[mask] = warped[mask]\n",
    "    return canvas\n",
    "\n",
    "# mosaic = sequential_image_stitching([img_1, img_2, img_3], [H12,H23])\n",
    "\n",
    "images = [img_1, img_2, img_3, img_4, img_5, img_6]\n",
    "homographies = [H12, H23, H34, H45, H56]\n",
    "\n",
    "for i in range(len(images)-1):\n",
    "    mosaic = sequential_image_stitching([images[i], images[i+1]], [homographies[i]])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image {i+1} + Image {i+2}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13daf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches16 = flann.knnMatch(desp1,desp6,k=2)\n",
    "matches25 = flann.knnMatch(desp2,desp5,k=2)\n",
    "matches24 = flann.knnMatch(desp2,desp4,k=2)\n",
    "matches35 = flann.knnMatch(desp3,desp5,k=2)\n",
    "matches15 = flann.knnMatch(desp1,desp5,k=2)\n",
    "matches26 = flann.knnMatch(desp2,desp6,k=2)\n",
    "\n",
    "sorted_matches16 = []\n",
    "sorted_matches25 = []\n",
    "sorted_matches24 = []\n",
    "sorted_matches35 = []\n",
    "sorted_matches15 = []\n",
    "sorted_matches26 = []\n",
    "\n",
    "\n",
    "matches = [matches12,matches23,matches34,matches45,matches56]\n",
    "\n",
    "for i,j in matches16:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches16.append(i)\n",
    "for i,j in matches25:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches25.append(i)\n",
    "for i,j in matches24:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches24.append(i)\n",
    "for i,j in matches35:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches35.append(i)\n",
    "for i,j in matches15:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches15.append(i)\n",
    "for i,j in matches16:\n",
    "            if i.distance<0.7*j.distance:\n",
    "                sorted_matches16.append(i)\n",
    "\n",
    "\n",
    "im16_matches = cv2.drawMatches(img_1, fp1, img_6, fp6, sorted_matches16, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im25_matches = cv2.drawMatches(img_2, fp2, img_5, fp5, sorted_matches25, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im24_matches = cv2.drawMatches(img_2, fp2, img_4, fp4, sorted_matches24, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im35_matches = cv2.drawMatches(img_3, fp3, img_5, fp5, sorted_matches35, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im15_matches = cv2.drawMatches(img_1, fp1, img_5, fp5, sorted_matches15, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "im26_matches = cv2.drawMatches(img_2, fp2, img_6, fp6, sorted_matches26, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "\n",
    "\n",
    "matched16_xy = []\n",
    "matched25_xy = []\n",
    "matched24_xy = []\n",
    "matched35_xy = []\n",
    "matched15_xy = []\n",
    "matched26_xy = []\n",
    "\n",
    "for match in sorted_matches16:\n",
    "    pt1 = fp1[match.queryIdx].pt\n",
    "    pt6 = fp6[match.trainIdx].pt\n",
    "    matched16_xy.append((pt1, pt6))\n",
    "for match in sorted_matches25:\n",
    "    pt2 = fp2[match.queryIdx].pt\n",
    "    pt5 = fp5[match.trainIdx].pt\n",
    "    matched25_xy.append((pt2, pt5))\n",
    "for match in sorted_matches24:\n",
    "    pt2 = fp2[match.queryIdx].pt\n",
    "    pt4 = fp4[match.trainIdx].pt\n",
    "    matched24_xy.append((pt2, pt4))\n",
    "for match in sorted_matches35:\n",
    "    pt3 = fp3[match.queryIdx].pt\n",
    "    pt5 = fp5[match.trainIdx].pt\n",
    "    matched35_xy.append((pt3, pt5))\n",
    "for match in sorted_matches15:\n",
    "    pt1 = fp1[match.queryIdx].pt\n",
    "    pt5 = fp5[match.trainIdx].pt\n",
    "    matched15_xy.append((pt1, pt5))\n",
    "for match in sorted_matches26:\n",
    "    pt2 = fp2[match.queryIdx].pt\n",
    "    pt6 = fp6[match.trainIdx].pt\n",
    "    matched26_xy.append((pt2, pt6))\n",
    "\n",
    "#tried finding a match between image 2to5 and even from image 1 to 5 and 2 to 6 but didnt get any matches \n",
    "\n",
    "\n",
    "src_pts16,dst_pts16 = pts(matched16_xy)\n",
    "src_pts25,dst_pts25 = pts(matched25_xy)\n",
    "src_pts24,dst_pts24 = pts(matched24_xy)\n",
    "src_pts35,dst_pts35 = pts(matched35_xy)\n",
    "src_pts15,dst_pts15 = pts(matched15_xy)\n",
    "src_pts26,dst_pts26 = pts(matched26_xy)\n",
    "\n",
    "matched = [matched16_xy, matched25_xy, matched24_xy, matched35_xy, matched15_xy, matched26_xy]\n",
    "\n",
    "# print(len(matched[1]))\n",
    "\n",
    "if len(matched[0]) >= 4:\n",
    "    H16, mask16 = cv2.findHomography(src_pts16, dst_pts16, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[0], images[5]], [H16])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image 1 + Image 6')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 16\")\n",
    "\n",
    "if len(matched[1]) >= 4:\n",
    "    H25, mask25 = cv2.findHomography(src_pts25, dst_pts25, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[1], images[4]], [H25])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image 2 + Image 5')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 25\")\n",
    "\n",
    "if len(matched[2]) >= 4:\n",
    "    H24, mask24 = cv2.findHomography(src_pts24, dst_pts24, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[1], images[3]], [H24])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image {1+1} + Image {2+2}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 24\")\n",
    "\n",
    "if len(matched[3]) >= 4:\n",
    "    H35, mask35 = cv2.findHomography(src_pts35, dst_pts35, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[2], images[4]], [H35])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image {2+1} + Image {3+2}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 35\")\n",
    "\n",
    "if len(matched[4]) >= 4:\n",
    "    H15, mask15 = cv2.findHomography(src_pts15, dst_pts15, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[0], images[4]], [H15])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image {0+1} + Image {3+2}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 15\")\n",
    "\n",
    "if len(matched[5]) >= 4:\n",
    "    H26, mask26 = cv2.findHomography(src_pts26, dst_pts26, cv2.RANSAC)\n",
    "    mosaic = sequential_image_stitching([images[1], images[5]], [H26])\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(mosaic, cmap='gray')\n",
    "    plt.title(f'Mosaic: Image {1+1} + Image {4+2}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not enough points to really match 26\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacd51b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39829cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff6097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
